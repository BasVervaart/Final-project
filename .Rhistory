babynames %>%
group_by(name) %>%
sum(n) %>%
summarize(n())
babynames %>%
group_by(name) %>%
summarize(n())
babynames %>%
group_by(name) %>%
summarise(total=sum(n)) %>%
arrange(total)
babynames %>%
group_by(name) %>%
summarise(total=sum(n)) %>%
arrange(desc(total))
kim <- babynames %>%
filter(name="Kim") %>%
group_by(sex)
kim <- babynames %>%
filter(name=="Kim") %>%
group_by(sex)
View(kim)
kim2<- babynames %>%
group_by(sex) %>%
filter(name==c("Kim")) %>%
kim2<- babynames %>%
group_by(sex) %>%
filter(name==c("Kim"))
kim2 <- babynames %>%
group_by(sex) %>%
filter(name==c("Kim"))
View(kim2)
kim <- babynames %>%
select(year,sex,name,n) %>%
filter(name==c("Kim")) %>%
group_by(sex)
kim.plot <- ggplot(data=kim,aes(x=year,y=n,fill=sex)) +
geom_line()
library(ggplot2)
kim.plot <- ggplot(data=kim,aes(x=year,y=n,fill=sex)) +
geom_line()
kim.plot
kim.plot <- ggplot(data=kim,aes(x=year,y=n,linetype=sex)) +
geom_point(mapping=aes(color=sex)) +
geom_smooth()
kim.plot
kim.plot <- ggplot(data=kim,aes(x=year,y=n)) +
geom_point(mapping=aes(color=sex)) +
geom_smooth()
kim.plot
kim.plot <- ggplot(data=kim,aes(x=year,y=n,linetype=sex)) +
geom_point(mapping=aes(color=sex))
kim.plot
kim.plot <- ggplot(data=kim,aes(x=year,y=n,linetype=sex)) +
geom_smooth(mapping=aes(color=sex))
kim.plot
kim.plot <- ggplot(data=kim,aes(x=year,y=n,linetype=sex)) +
geom_smooth(mapping=aes(color=sex),se=F)
kim.plot
kim.plot <- ggplot(data=kim,aes(x=year,y=n,linetype=sex)) +
geom_smooth(mapping=aes(color=sex),se=F) +
labs(title="Frequency of the name Kim by sex")
kim.plot
kim.plot <- ggplot(data=kim,aes(x=year,y=n,linetype=sex)) +
geom_smooth(mapping=aes(color=sex),se=F) +
labs(title="Frequency of the name Kim by sex") +
locator()
kim.plot
locator(kim.plot)
intersect <- approxfun(kim.plot$value, kim.plot$time)
intersect <- approxintersection(kim.plot$value, kim.plot$time)
kim <- babynames %>%
select(year,sex,name,n) %>%
mutate(log_n=log(n)) %>%
filter(name==c("Kim")) %>%
group_by(sex)
View(kim)
kim.plot <- ggplot(data=kim,aes(x=year,y=log_n,linetype=sex)) +
geom_smooth(mapping=aes(color=sex),se=F) +
labs(title="Frequency of the name Kim by sex")
kim.plot
View(kim2)
kim <- babynames %>%
select(year,sex,name,n) %>%
filter(name==c("Kim")) %>%
group_by(sex)
library(ggplot2)
kim.plot <- ggplot(data=kim,aes(x=year,y=n,linetype=sex)) +
geom_smooth(mapping=aes(color=sex),se=F) +
labs(title="Frequency of the name Kim by sex")
kim.plot
closeAllConnections()
closeAllConnections()
setwd("C:/Users/Bas Vervaart/Documents/Web data collection and social media mining/Final project")
write.csv(publications, "hertie_publications.csv")
write.csv(hertie_sentiment, "hertie_sentiment.csv")
write.csv(article_languages, "hertie_article_languages.csv")
#Headlines
hertie <- read_html("hertie_full.html")
headlines<- html_nodes(hertie,css=".grid-item-title") %>% html_text()
#Headlines Cleaned
headlines<-str_replace_all(headlines,"\\u0092|\\u0091","'") %>%
str_replace_all(.,"\\u0093|\\u0094|\\u0096|\\u0097",'"') %>%
gsub('\"', "",., fixed = TRUE)
#Links to all articles
links<- html_nodes(hertie,xpath="//a") %>% html_attr("href") %>%  as.list()
islink <- str_detect(links, "/en/debate/in-the-media/detail/content/")
regexp <- ".*"
article_list <- lapply(links[islink], str_extract, regexp)
base_url <- 'https://www.hertie-school.org'
url_list <- paste0(base_url,article_list)
try<- str_detect(links, "/en/debate/in-the-media/")
try_list <- lapply(links[islink], str_extract, regexp)
#The first two links still need to be cleaned
two_rows <- url_list[1:2] %>% str_replace('https://www.hertie-school.org',"")
url_list <- url_list[3:340]
url_list <- append(two_rows, url_list) %>% as.list()
###########
#List of authors
author_list <- character()
author_list <- lapply(url_list, function(i){
webpage <- read_html(i)
nodes <- html_nodes(webpage, xpath='//*[contains(concat( " ", @class, " " ), concat( " ", "style-small", " " ))]//*[contains(concat( " ", @class, " " ), concat( " ", "text-cell", " " ))]')
author_text <- html_text(nodes)
})
author_list_backup<- author_list
author_list <- author_list_backup
#Cleaning author_list
author_list<- author_list[lengths(author_list) > 0L]
author_list_two <- author_list %>% str_replace_all("[\t\n]" , "") %>% str_trim() %>%
str_split(",(?=[^,]+$)")
author_list_df <- do.call(rbind,author_list_two)
colnames(author_list_df) <- c("Name","Position")
author_list_df <- as.data.frame(author_list_df)
author_list_df$Name<- as.character(author_list_df$Name)
author_list_df$Position<- as.character(author_list_df$Position)
author_list_df[11,c(1,2)] <- c("Johanna Mair", "Professor for Organization, Strategy and Leadership")
#Text on Hertie page, turns out text also includes the author, so we can just put them together in one dataframe
text <- character()
text <- lapply(url_list, function(i){
webpage <- read_html(i)
nodes <- html_nodes(webpage, xpath='//*[contains(concat( " ", @class, " " ), concat( " ", "large-text", " " ))]//p | //*[contains(concat( " ", @class, " " ), concat( " ", "style-small", " " ))]//*[contains(concat( " ", @class, " " ), concat( " ", "text-cell", " " ))]')
text_text <- html_text(nodes)
})
text_clean <- lapply(text, function(x) {str_replace_all(x, "[\t\n]" , "")})
text_df<- as.data.frame(t(stri_list2matrix(text_clean)))
columns<-str_split_fixed(text_df$V2, ",", 2)
text_df<-cbind(text_df,columns)
text_df[,2] <- NULL
colnames(text_df)<- c("Content","Author","Position")
freqfunc <- function(x, n){
head(sort(table(unlist(strsplit(as.character(x), ", "))),decreasing=T),n)
}
freqfunc(author_list_df$Name,3) #Three most common published authors
#Use text of articles
authors<- c("Anheier","Bernoth","Çalı","Cingolani","Dawson","Enderlein","Flachsland","Graf",
"Hallerberg","Hammerschmid","Hassel","Hirth","Hurrelmann","Ischinger","Jachtenfuchs",
"Joerges","Kayser","Kemfert","Kreyenfeld","Mair","Mungiu-Pippidi","Pisani-Ferry","Römmele",
"Schwander","Stockmann","Traxler","Wegrich","Wucherpfennig","Munzert")
library(tidyverse)
publications<-map_df(authors, ~ str_count(text_df$Content, .x) %>%
sum %>%
set_names(.x) %>%
enframe(name = "Author", value = "count"))
publications<-publications[order(publications$count,decreasing=T),]
head(publications)
#Or graphically
ggplot(data=publications,mapping=aes(x=reorder(Author,count),y=count,fill=Author)) +
geom_bar(stat='identity') +
coord_flip() +
labs(x="Professor",y="Count") +
ggtitle("Number of publications by Professor") +
theme_bw()
#How many articles in German vs English
ingerman <- "in German"
map_df(ingerman, ~ str_count(text_df$Content, .x) %>%
sum %>%
set_names(.x) %>%
enframe(value = "count"))
#167 articles are in German, that means 173 are not
Language <- c("German","English")
Count <- c(167,173)
article_languages <- as.data.frame(cbind(Language,Count))
article_languages$Count <-  as.numeric(as.character(article_languages$Count))
article_languages<- article_languages %>% mutate(percentage = Count/sum(Count))
ggplot(data=article_languages,mapping=aes(x=Language, y=percentage,fill=Language)) +
geom_bar(stat='identity') +
labs(x="Language",y="Count") +
ggtitle("Percentage of articles in English vs German") +
theme_bw()
###Text analysis, most common words/themes
library(tidytext)
text_analysis <- text_analysis %>% unnest_tokens(word, text)
common_words<-text_df$Content %>%
na.omit() %>%
tolower() %>%
strsplit(split = "\\W") %>% # or strsplit(split = "\\W")
unlist() %>%
table() %>%
sort(decreasing = TRUE)
head(common_words)
headlines_count <- data_frame(line = 1:343, text = headlines) %>% unnest_tokens(word,text) %>%
anti_join(stop_words) %>% count(word,sort=T)
#Most common used words in Hertie headlines
headlines_analysis <- data_frame(line = 1:343, text = headlines) %>% unnest_tokens(word, text)
headlines_analysis %>%
count(word, sort = TRUE)
#Get rid of the stop words
data(stop_words)
(headlines_count <- headlines_analysis %>%
anti_join(stop_words) %>% count(word,sort=T))
#Seems the school is after all much focused on Germany and Europe
headlines_count%>%
filter(n>10) %>%
ggplot(mapping=aes(x=reorder(word, n),y=n)) +
geom_col() +
xlab(NULL) +
coord_flip()
#How positive vs negative are Hertie headlines?
library(tidyr)
#Get sentiment using the Bing lexicon
hertie_sentiment <- headlines_analysis %>%
inner_join(get_sentiments("bing")) %>%
count(word, index = line, sentiment) %>%
spread(sentiment, n, fill = 0) %>%
mutate(sentiment = positive - negative)
hertie_sentiment %>%
ggplot(aes(index, sentiment, fill = sentiment)) +
geom_bar(alpha = 0.5, stat = "identity", show.legend = FALSE) +
ggtitle("Sentiment analysis of Hertie headlines")+
theme_bw()
(word_count <- headlines_analysis %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort = TRUE) %>%
ungroup())
#Turns out Hertie staff liks to use the word crisis in headlines and is concerned with Trump,
#but whether that's actually positive. (Too obsessed with Trump?)
#Otherwise common words we expect from a policy school, 'right' could refer to political
#center as well as the legal definition of a right. Freedom and corrupt are also much in the policy domain
setwd("C:/Users/Bas Vervaart/Documents/Web data collection and social media mining/Final project")
write.csv(publications, "hertie_publications.csv")
write.csv(hertie_sentiment, "hertie_sentiment.csv")
write.csv(article_languages, "hertie_article_languages.csv")
library(xml2)
library(tidyverse)
library(stringr)
library(RSelenium)
library(rvest)
#Headlines
hertie <- read_html("hertie_full.html")
headlines<- html_nodes(hertie,css=".grid-item-title") %>% html_text()
#Headlines Cleaned
headlines<-str_replace_all(headlines,"\\u0092|\\u0091","'") %>%
str_replace_all(.,"\\u0093|\\u0094|\\u0096|\\u0097",'"') %>%
gsub('\"', "",., fixed = TRUE)
#Links to all articles
links<- html_nodes(hertie,xpath="//a") %>% html_attr("href") %>%  as.list()
islink <- str_detect(links, "/en/debate/in-the-media/detail/content/")
regexp <- ".*"
article_list <- lapply(links[islink], str_extract, regexp)
base_url <- 'https://www.hertie-school.org'
url_list <- paste0(base_url,article_list)
try<- str_detect(links, "/en/debate/in-the-media/")
try_list <- lapply(links[islink], str_extract, regexp)
#The first two links still need to be cleaned
two_rows <- url_list[1:2] %>% str_replace('https://www.hertie-school.org',"")
url_list <- url_list[3:340]
url_list <- append(two_rows, url_list) %>% as.list()
###########
#List of authors
author_list <- character()
author_list <- lapply(url_list, function(i){
webpage <- read_html(i)
nodes <- html_nodes(webpage, xpath='//*[contains(concat( " ", @class, " " ), concat( " ", "style-small", " " ))]//*[contains(concat( " ", @class, " " ), concat( " ", "text-cell", " " ))]')
author_text <- html_text(nodes)
})
author_list_backup<- author_list
author_list <- author_list_backup
#Cleaning author_list
author_list<- author_list[lengths(author_list) > 0L]
author_list_two <- author_list %>% str_replace_all("[\t\n]" , "") %>% str_trim() %>%
str_split(",(?=[^,]+$)")
author_list_df <- do.call(rbind,author_list_two)
colnames(author_list_df) <- c("Name","Position")
author_list_df <- as.data.frame(author_list_df)
author_list_df$Name<- as.character(author_list_df$Name)
author_list_df$Position<- as.character(author_list_df$Position)
author_list_df[11,c(1,2)] <- c("Johanna Mair", "Professor for Organization, Strategy and Leadership")
#Text on Hertie page, turns out text also includes the author, so we can just put them together in one dataframe
text <- character()
text <- lapply(url_list, function(i){
webpage <- read_html(i)
nodes <- html_nodes(webpage, xpath='//*[contains(concat( " ", @class, " " ), concat( " ", "large-text", " " ))]//p | //*[contains(concat( " ", @class, " " ), concat( " ", "style-small", " " ))]//*[contains(concat( " ", @class, " " ), concat( " ", "text-cell", " " ))]')
text_text <- html_text(nodes)
})
text_clean <- lapply(text, function(x) {str_replace_all(x, "[\t\n]" , "")})
text_df<- as.data.frame(t(stri_list2matrix(text_clean)))
columns<-str_split_fixed(text_df$V2, ",", 2)
text_df<-cbind(text_df,columns)
text_df[,2] <- NULL
colnames(text_df)<- c("Content","Author","Position")
freqfunc <- function(x, n){
head(sort(table(unlist(strsplit(as.character(x), ", "))),decreasing=T),n)
}
freqfunc(author_list_df$Name,3) #Three most common published authors
#Use text of articles
authors<- c("Anheier","Bernoth","Çalı","Cingolani","Dawson","Enderlein","Flachsland","Graf",
"Hallerberg","Hammerschmid","Hassel","Hirth","Hurrelmann","Ischinger","Jachtenfuchs",
"Joerges","Kayser","Kemfert","Kreyenfeld","Mair","Mungiu-Pippidi","Pisani-Ferry","Römmele",
"Schwander","Stockmann","Traxler","Wegrich","Wucherpfennig","Munzert")
library(tidyverse)
publications<-map_df(authors, ~ str_count(text_df$Content, .x) %>%
sum %>%
set_names(.x) %>%
enframe(name = "Author", value = "count"))
publications<-publications[order(publications$count,decreasing=T),]
head(publications)
#Or graphically
ggplot(data=publications,mapping=aes(x=reorder(Author,count),y=count,fill=Author)) +
geom_bar(stat='identity') +
coord_flip() +
labs(x="Professor",y="Count") +
ggtitle("Number of publications by Professor") +
theme_bw()
#How many articles in German vs English
ingerman <- "in German"
map_df(ingerman, ~ str_count(text_df$Content, .x) %>%
sum %>%
set_names(.x) %>%
enframe(value = "count"))
#167 articles are in German, that means 173 are not
Language <- c("German","English")
Count <- c(167,173)
article_languages <- as.data.frame(cbind(Language,Count))
article_languages$Count <-  as.numeric(as.character(article_languages$Count))
article_languages<- article_languages %>% mutate(percentage = Count/sum(Count))
ggplot(data=article_languages,mapping=aes(x=Language, y=percentage,fill=Language)) +
geom_bar(stat='identity') +
labs(x="Language",y="Count") +
ggtitle("Percentage of articles in English vs German") +
theme_bw()
###Text analysis, most common words/themes
library(tidytext)
text_analysis <- text_analysis %>% unnest_tokens(word, text)
common_words<-text_df$Content %>%
na.omit() %>%
tolower() %>%
strsplit(split = "\\W") %>% # or strsplit(split = "\\W")
unlist() %>%
table() %>%
sort(decreasing = TRUE)
head(common_words)
headlines_count <- data_frame(line = 1:343, text = headlines) %>% unnest_tokens(word,text) %>%
anti_join(stop_words) %>% count(word,sort=T)
#Most common used words in Hertie headlines
headlines_analysis <- data_frame(line = 1:343, text = headlines) %>% unnest_tokens(word, text)
headlines_analysis %>%
count(word, sort = TRUE)
#Get rid of the stop words
data(stop_words)
(headlines_count <- headlines_analysis %>%
anti_join(stop_words) %>% count(word,sort=T))
#Seems the school is after all much focused on Germany and Europe
headlines_count%>%
filter(n>10) %>%
ggplot(mapping=aes(x=reorder(word, n),y=n)) +
geom_col() +
xlab(NULL) +
coord_flip()
#How positive vs negative are Hertie headlines?
library(tidyr)
#Get sentiment using the Bing lexicon
hertie_sentiment <- headlines_analysis %>%
inner_join(get_sentiments("bing")) %>%
count(word, index = line, sentiment) %>%
spread(sentiment, n, fill = 0) %>%
mutate(sentiment = positive - negative)
hertie_sentiment %>%
ggplot(aes(index, sentiment, fill = sentiment)) +
geom_bar(alpha = 0.5, stat = "identity", show.legend = FALSE) +
ggtitle("Sentiment analysis of Hertie headlines")+
theme_bw()
(word_count <- headlines_analysis %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort = TRUE) %>%
ungroup())
#Turns out Hertie staff liks to use the word crisis in headlines and is concerned with Trump,
#but whether that's actually positive. (Too obsessed with Trump?)
#Otherwise common words we expect from a policy school, 'right' could refer to political
#center as well as the legal definition of a right. Freedom and corrupt are also much in the policy domain
setwd("C:/Users/Bas Vervaart/Documents/Web data collection and social media mining/Final project")
write.csv(publications, "hertie_publications.csv")
write.csv(hertie_sentiment, "hertie_sentiment.csv")
write.csv(article_languages, "hertie_article_languages.csv")
text_df<- as.data.frame(t(stri_list2matrix(text_clean)))
columns<-str_split_fixed(text_df$V2, ",", 2)
text_df<-cbind(text_df,columns)
text_df[,2] <- NULL
colnames(text_df)<- c("Content","Author","Position")
library(stringi)
text_df<- as.data.frame(t(stri_list2matrix(text_clean)))
columns<-str_split_fixed(text_df$V2, ",", 2)
text_df<-cbind(text_df,columns)
text_df[,2] <- NULL
colnames(text_df)<- c("Content","Author","Position")
publications<-map_df(authors, ~ str_count(text_df$Content, .x) %>%
sum %>%
set_names(.x) %>%
enframe(name = "Author", value = "count"))
publications<-publications[order(publications$count,decreasing=T),]
head(publications)
ggplot(data=publications,mapping=aes(x=reorder(Author,count),y=count,fill=Author)) +
geom_bar(stat='identity') +
coord_flip() +
labs(x="Professor",y="Count") +
ggtitle("Number of publications by Professor") +
theme_bw()
#How many articles in German vs English
ingerman <- "in German"
map_df(ingerman, ~ str_count(text_df$Content, .x) %>%
sum %>%
set_names(.x) %>%
enframe(value = "count"))
#167 articles are in German, that means 173 are not
Language <- c("German","English")
Count <- c(167,173)
article_languages <- as.data.frame(cbind(Language,Count))
article_languages$Count <-  as.numeric(as.character(article_languages$Count))
article_languages<- article_languages %>% mutate(percentage = Count/sum(Count))
ggplot(data=article_languages,mapping=aes(x=Language, y=percentage,fill=Language)) +
geom_bar(stat='identity') +
labs(x="Language",y="Count") +
ggtitle("Percentage of articles in English vs German") +
theme_bw()
###Text analysis, most common words/themes
library(tidytext)
text_analysis <- text_analysis %>% unnest_tokens(word, text)
common_words<-text_df$Content %>%
na.omit() %>%
tolower() %>%
strsplit(split = "\\W") %>% # or strsplit(split = "\\W")
unlist() %>%
table() %>%
sort(decreasing = TRUE)
head(common_words)
headlines_count <- data_frame(line = 1:343, text = headlines) %>% unnest_tokens(word,text) %>%
anti_join(stop_words) %>% count(word,sort=T)
#Most common used words in Hertie headlines
headlines_analysis <- data_frame(line = 1:343, text = headlines) %>% unnest_tokens(word, text)
headlines_analysis %>%
count(word, sort = TRUE)
#Get rid of the stop words
data(stop_words)
(headlines_count <- headlines_analysis %>%
anti_join(stop_words) %>% count(word,sort=T))
#Seems the school is after all much focused on Germany and Europe
headlines_count%>%
filter(n>10) %>%
ggplot(mapping=aes(x=reorder(word, n),y=n)) +
geom_col() +
xlab(NULL) +
coord_flip()
#How positive vs negative are Hertie headlines?
library(tidyr)
#Get sentiment using the Bing lexicon
hertie_sentiment <- headlines_analysis %>%
inner_join(get_sentiments("bing")) %>%
count(word, index = line, sentiment) %>%
spread(sentiment, n, fill = 0) %>%
mutate(sentiment = positive - negative)
hertie_sentiment %>%
ggplot(aes(index, sentiment, fill = sentiment)) +
geom_bar(alpha = 0.5, stat = "identity", show.legend = FALSE) +
ggtitle("Sentiment analysis of Hertie headlines")+
theme_bw()
(word_count <- headlines_analysis %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort = TRUE) %>%
ungroup())
#Turns out Hertie staff liks to use the word crisis in headlines and is concerned with Trump,
#but whether that's actually positive. (Too obsessed with Trump?)
#Otherwise common words we expect from a policy school, 'right' could refer to political
#center as well as the legal definition of a right. Freedom and corrupt are also much in the policy domain
setwd("C:/Users/Bas Vervaart/Documents/Web data collection and social media mining/Final project")
write.csv(publications, "hertie_publications.csv")
write.csv(hertie_sentiment, "hertie_sentiment.csv")
write.csv(article_languages, "hertie_article_languages.csv")
View(hertie_sentiment)
write.csv(hertie_sentiment, "hertie_sentiment.csv")
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggplot2)
article_languages <- read.csv("hertie_article_languages.csv")
hertie_sentiment <- read.csv("hertie_sentiment.csv")
publications <- read.csv("hertie_publications.csv")
ggplot(data=hertie_sentiment,mapping= aes(index, sentiment, fill = sentiment)) +
geom_bar(alpha = 0.5, stat = "identity", show.legend = FALSE) +
ggtitle("Sentiment analysis of Hertie headlines")+
hertie_sentiment %>%
ggplot(aes(index, sentiment, fill = sentiment)) +
geom_bar(alpha = 0.5, stat = "identity", show.legend = FALSE) +
ggtitle("Sentiment analysis of Hertie headlines")+
theme_bw()
test<-read.csv("hertie_sentiment.csv")
test %>%
ggplot(aes(index, sentiment, fill = sentiment)) +
geom_bar(alpha = 0.5, stat = "identity", show.legend = FALSE) +
ggtitle("Sentiment analysis of Hertie headlines")+
theme_bw()
ggplot(data=hertie_sentiment,mapping= aes(index, sentiment, fill = sentiment)) +
geom_bar(alpha = 0.5, stat = "identity", show.legend = FALSE) +
ggtitle("Sentiment analysis of Hertie headlines")
